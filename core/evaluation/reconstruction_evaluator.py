"""Evaluator for comparing event reconstruction methods."""

import numpy as np
from typing import Union, Optional, List, Tuple, Callable
import matplotlib.pyplot as plt
import os
import timeit
import keras as keras
from core.reconstruction import (
    EventReconstructorBase,
    GroundTruthReconstructor,
    KerasFFRecoBase,
    CompositeNeutrinoComponentReconstructor,
)
import seaborn as sns
from core.base_classes import KerasMLWrapper
from .evaluator_base import (
    PlotConfig,
    BootstrapCalculator,
    BinningUtility,
    FeatureExtractor,
    AccuracyCalculator,
    SelectionAccuracyCalculator,
    NeutrinoDeviationCalculator,
)
from .plotting_utils import (
    AccuracyPlotter,
    ConfusionMatrixPlotter,
    ResolutionPlotter,
    NeutrinoDeviationPlotter,
    SelectionAccuracyPlotter,
    DistributionPlotter,
)

from .physics_calculations import (
    TopReconstructor,
    ResolutionCalculator,
    c_hel,
    c_han,
)
from .reco_variable_config import reconstruction_variable_configs

from core.utils import (
    compute_pt_from_lorentz_vector_array,
    project_vectors_onto_axis,
    lorentz_vector_from_PtEtaPhiE_array,
)


class PredictionManager:
    """Manages predictions from multiple reconstructors."""

    def __init__(
        self,
        reconstructors: Union[EventReconstructorBase, List[EventReconstructorBase]],
        X_test: dict,
        y_test: dict,
        load_directory: Optional[str] = None,
    ):
        # Handle single reconstructor
        if isinstance(reconstructors, EventReconstructorBase):
            reconstructors = [reconstructors]

        self.reconstructors = reconstructors
        self.X_test = X_test
        self.y_test = y_test
        self.predictions = []
        if load_directory is not None:
            self.load_predictions(load_directory)
        else:
            self._compute_all_predictions()

    def _compute_all_predictions(self):
        """Compute predictions for all reconstructors."""
        for reconstructor in self.reconstructors:
            if isinstance(reconstructor, KerasFFRecoBase):
                assignment_pred, neutrino_regression = (
                    reconstructor.complete_forward_pass(self.X_test)
                )
                self.predictions.append(
                    {
                        "assignment": assignment_pred,
                        "regression": neutrino_regression,
                    }
                )
            else:
                assignment_pred = reconstructor.predict_indices(self.X_test)
                if hasattr(reconstructor, "reconstruct_neutrinos"):
                    neutrino_pred = reconstructor.reconstruct_neutrinos(self.X_test)
                else:
                    print(
                        "WARNING: Reconstructor does not support neutrino regression."
                    )
                    neutrino_pred = None
                self.predictions.append(
                    {
                        "assignment": assignment_pred,
                        "regression": neutrino_pred,
                    }
                )
            keras.backend.clear_session(free_memory=True)

    def save_predictions(self, output_dir: str):
        """Save predictions to the specified output directory."""
        os.makedirs(output_dir, exist_ok=True)

        for idx, reconstructor in enumerate(self.reconstructors):
            reconstructor_dir = os.path.join(
                output_dir, reconstructor.get_full_reco_name().replace(" ", "_")
            )
            os.makedirs(reconstructor_dir, exist_ok=True)

            assignment_path = os.path.join(
                reconstructor_dir, "assignment_predictions.npz"
            )
            np.savez(
                assignment_path,
                predictions=self.predictions[idx]["assignment"],
            )

            if self.predictions[idx]["regression"] is not None:
                regression_path = os.path.join(
                    reconstructor_dir, "neutrino_regression_predictions.npz"
                )
                np.savez(
                    regression_path,
                    predictions=self.predictions[idx]["regression"],
                )
            print(f"Predictions saved for {reconstructor.get_full_reco_name()}.")

        np.savez(
            os.path.join(output_dir, "event_indices.npz"),
            event_indices=FeatureExtractor.get_event_indices(self.X_test),
        )

    def load_predictions(self, input_dir: str):
        """Load predictions from the specified input directory."""
        for idx, reconstructor in enumerate(self.reconstructors):
            reconstructor_dir = os.path.join(
                input_dir, reconstructor.get_full_reco_name().replace(" ", "_")
            )

            assignment_path = os.path.join(
                reconstructor_dir, "assignment_predictions.npz"
            )
            assignment_data = np.load(assignment_path)
            assignment_predictions = assignment_data["predictions"]

            regression_path = os.path.join(
                reconstructor_dir, "neutrino_regression_predictions.npz"
            )
            if os.path.exists(regression_path):
                regression_data = np.load(regression_path)
                regression_predictions = regression_data["predictions"]
            else:
                regression_predictions = None

            self.predictions.append(
                {
                    "assignment": assignment_predictions,
                    "regression": regression_predictions,
                }
            )
            print(f"Predictions loaded for {reconstructor.get_full_reco_name()}.")
        event_indices_path = os.path.join(input_dir, "event_indices.npz")
        event_indices_data = np.load(event_indices_path)
        loaded_event_indices = event_indices_data["event_indices"]
        if "event_number" not in self.X_test:
            raise ValueError(
                "Event indices not found in X_test. " "Cannot align loaded predictions."
            )
        current_event_indices = FeatureExtractor.get_event_indices(self.X_test)
        if not np.array_equal(loaded_event_indices, current_event_indices):
            shared_event_indicies = np.union1d(
                loaded_event_indices, current_event_indices
            )

    def get_assignment_predictions(self, index: int) -> np.ndarray:
        """Get assignment predictions for a specific reconstructor."""
        return self.predictions[index]["assignment"]

    def get_neutrino_predictions(self, index: int) -> np.ndarray:
        """Get neutrino predictions for a specific reconstructor."""
        return self.predictions[index]["regression"]


class ComputationTimeEvaluator:
    """Evaluator for measuring computation time of reconstructors."""

    def __init__(
        self,
        reconstructors: List[EventReconstructorBase],
        X_test: dict,
    ):
        self.reconstructors = reconstructors
        self.X_test = X_test

    def evaluate_computation_times(self) -> List[float]:
        """Evaluate computation times for all reconstructors."""
        times = []
        for reconstructor in self.reconstructors:
            start_time = timeit.default_timer()
            reconstructor.predict_indices(self.X_test)
            end_time = timeit.default_timer()
            times.append(end_time - start_time)
        return times


class ComplementarityAnalyzer:
    """Analyzes complementarity between reconstructors."""

    def __init__(
        self,
        prediction_manager: PredictionManager,
        y_test: dict,
    ):
        self.prediction_manager = prediction_manager
        self.y_test = y_test

    def compute_complementarity_matrix(self) -> np.ndarray:
        """
        Compute complementarity matrix between reconstructors.

        Returns:
            Complementarity matrix (n_reconstructors, n_reconstructors)
        """

        per_event_accuracies = self._get_all_per_event_accuracies()

        n_reconstructors = len(per_event_accuracies)
        complementarity_matrix = np.zeros((n_reconstructors, n_reconstructors))

        for i in range(n_reconstructors):
            for j in range(n_reconstructors):
                if i != j:
                    both_incorrect = np.sum(
                        (1 - per_event_accuracies[i]) * (1 - per_event_accuracies[j])
                    )
                    total_events = len(per_event_accuracies[i])
                    complementarity_matrix[i, j] = both_incorrect / total_events

        return complementarity_matrix

    def get_per_event_complementarity(self) -> np.ndarray:
        """
        Get per-event success for at least one reconstructor.

        Returns:
            Array of per-event complementarity (n_events,)
        """
        per_event_accuracies = self._get_all_per_event_accuracies()
        per_event_success = np.stack(per_event_accuracies, axis=1).astype(bool)
        return np.any(per_event_success, axis=1).astype(float)

    def _get_all_per_event_accuracies(self) -> List[np.ndarray]:
        """Get per-event accuracies for all reconstructors."""
        accuracies = []
        for i in range(len(self.prediction_manager.reconstructors)):
            if isinstance(
                self.prediction_manager.reconstructors[i],
                GroundTruthReconstructor,
            ):
                continue
            predictions = self.prediction_manager.get_assignment_predictions(i)
            accuracy_data = AccuracyCalculator.compute_accuracy(
                self.y_test["assignment_labels"],
                predictions,
                per_event=True,
            )
            accuracies.append(accuracy_data)
        return accuracies


class ReconstructionVariableHandler:
    """Handles configuration for reconstructed physics variables."""

    def __init__(
        self, variable_config, prediction_manager: PredictionManager, X_test: dict
    ):
        self.reco_variable_cache = {}
        self.prediction_manager = prediction_manager
        self.X_test = X_test
        self.configs = variable_config

    def compute_reconstructed_variable(
        self,
        reconstructor_index: int,
        variable_name: str,
    ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        Compute any reconstructed variable from event kinematics.

        Args:
            reconstructor_index: Index of the reconstructor
            variable_func: Function that takes (top1_p4, top2_p4, lepton_features, jet_features, neutrino_pred)
                          and returns the reconstructed variable(s)
            truth_extractor: Optional function to extract truth values from X_test
            combine_tops: If True, concatenate results for both tops (for per-top quantities)

        Returns:
            Reconstructed variable array or tuple of (reconstructed, truth) if truth_extractor provided
        """
        if variable_name not in self.configs:
            raise ValueError(f"Variable '{variable_name}' not found in configurations.")

        if (
            variable_name in self.reco_variable_cache
            and reconstructor_index in self.reco_variable_cache[variable_name]
        ):
            print(
                f"Using cached reconstructed variable '{variable_name}' for reconstructor {self.prediction_manager.reconstructors[reconstructor_index].get_full_reco_name()}."
            )
            return self.reco_variable_cache[variable_name][reconstructor_index]

        variable_func = self.configs[variable_name]["compute_func"]

        assignment_pred = self.prediction_manager.get_assignment_predictions(
            reconstructor_index
        )
        neutrino_pred = self.prediction_manager.get_neutrino_predictions(
            reconstructor_index
        )
        valid_events_mask = np.all(~np.isnan(neutrino_pred), axis=(1, 2)) & np.all(
            np.isfinite(neutrino_pred), axis=(1, 2)
        )
        if not np.all(valid_events_mask):
            print(
                f"Warning: NaN or infinite neutrino predictions found for reconstructor {self.prediction_manager.reconstructors[reconstructor_index].get_full_reco_name()}. These events will be skipped in variable computation."
            )
            neutrino_pred[~valid_events_mask] = 1 

        lepton_features = self.X_test[
            "lep_inputs"
        ]

        jet_features = self.X_test["jet_inputs"][
            :, :, :4
        ]
        selected_jet_indices = assignment_pred.argmax(axis=-2)
        reco_jets = np.take_along_axis(
            jet_features,
            selected_jet_indices[:, :, np.newaxis],
            axis=1,
        )

        reconstructed = variable_func(lepton_features, reco_jets, neutrino_pred)

        if isinstance(reconstructed, tuple):
            reconstructed = np.concatenate(reconstructed)

        if variable_name not in self.reco_variable_cache:
            self.reco_variable_cache[variable_name] = {}

        self.reco_variable_cache[variable_name][reconstructor_index] = reconstructed

        return reconstructed

    def compute_true_variable(
        self,
        variable_name: str,
    ) -> np.ndarray:
        """
        Compute any truth variable from X_test.

        Args:
            truth_extractor: Function to extract truth values from X_test
            combine_tops: If True, concatenate results for both tops (for per-top quantities)
        Returns:
            Truth variable array
        """
        combine_tops = self.configs[variable_name]["combine_tops"]
        if variable_name not in self.configs:
            raise ValueError(f"Variable '{variable_name}' not found in configurations.")

        truth_cache_key = f"{variable_name}_truth"
        if truth_cache_key in self.reco_variable_cache:
            return self.reco_variable_cache[truth_cache_key]

        truth_extractor = self.configs[variable_name]["extract_func"]

        truth = truth_extractor(self.X_test)

        if combine_tops and isinstance(truth, tuple):
            truth = np.concatenate(truth)

        truth_cache_key = f"{variable_name}_truth"
        self.reco_variable_cache[truth_cache_key] = truth

        return truth


class ReconstructionEvaluator:
    """Evaluator for comparing event reconstruction methods."""

    def __init__(
        self,
        prediction_manager: PredictionManager,
    ):
        self.variable_configs = reconstruction_variable_configs

        self.prediction_manager = prediction_manager

        self.X_test = prediction_manager.X_test
        self.y_test = prediction_manager.y_test

        self.config = self.prediction_manager.reconstructors[0].config

        self.variable_handler = ReconstructionVariableHandler(
            reconstruction_variable_configs, prediction_manager, self.X_test
        )

    def _validate_configs(self):
        """Validate that all reconstructors have the same configuration."""
        configs = [r.config for r in self.prediction_manager.reconstructors]
        base_config = configs[0]

        for config in configs[1:]:
            if config != base_config:
                raise ValueError(
                    "All reconstructors must have the same DataConfig for "
                    "consistent evaluation."
                )

    # ==================== Accuracy Methods ====================

    def evaluate_accuracy(
        self,
        reconstructor_index: int,
        per_event: bool = False,
    ) -> Union[float, np.ndarray]:
        """Evaluate accuracy for a specific reconstructor."""
        predictions = self.prediction_manager.get_assignment_predictions(
            reconstructor_index
        )
        return AccuracyCalculator.compute_accuracy(
            self.y_test["assignment_labels"],
            predictions,
            per_event=per_event,
        )

    def evaluate_selection_accuracy(
        self,
        reconstructor_index: int,
        per_event: bool = False,
    ) -> Union[float, np.ndarray]:
        """Evaluate selection accuracy for a specific reconstructor."""
        predictions = self.prediction_manager.get_assignment_predictions(
            reconstructor_index
        )
        return SelectionAccuracyCalculator.compute_selection_accuracy(
            self.y_test["assignment_labels"],
            predictions,
            per_event=per_event,
        )

    def _bootstrap_accuracy(
        self,
        reconstructor_index: int,
        config: PlotConfig,
    ) -> Tuple[float, float, float]:
        """Compute accuracy with bootstrap confidence intervals."""
        accuracy_data = self.evaluate_accuracy(reconstructor_index, per_event=True)
        return BootstrapCalculator.compute_bootstrap_ci(
            accuracy_data,
            n_bootstrap=config.n_bootstrap,
            confidence=config.confidence,
        )

    def _bootstrap_selection_accuracy(
        self,
        reconstructor_index: int,
        config: PlotConfig,
    ) -> Tuple[float, float, float]:
        """Compute selection accuracy with bootstrap confidence intervals."""
        accuracy_data = self.evaluate_selection_accuracy(
            reconstructor_index, per_event=True
        )
        return BootstrapCalculator.compute_bootstrap_ci(
            accuracy_data,
            n_bootstrap=config.n_bootstrap,
            confidence=config.confidence,
        )

    def plot_all_accuracies(
        self,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        figsize: Tuple[int, int] = (10, 6),
    ):
        """Plot accuracies for all reconstructors with error bars."""
        config = PlotConfig(
            figsize=figsize,
            confidence=confidence,
            n_bootstrap=n_bootstrap,
        )

        print("\nComputing bootstrap confidence intervals...")
        accuracies = []

        names = []
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                print(f"{reconstructor.get_assignment_name()}: Ground Truth (skipping)")
                continue
            mean_acc, lower, upper = self._bootstrap_accuracy(i, config)
            accuracies.append((mean_acc, lower, upper))
            print(
                f"{reconstructor.get_assignment_name()}: {mean_acc:.4f} "
                f"[{lower:.4f}, {upper:.4f}]"
            )
            names.append(reconstructor.get_assignment_name())

        return AccuracyPlotter.plot_overall_accuracies(names, accuracies, config)

    def plot_all_selection_accuracies(
        self,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        figsize: Tuple[int, int] = (10, 6),
    ):
        """Plot accuracies for all reconstructors with error bars."""
        config = PlotConfig(
            figsize=figsize,
            confidence=confidence,
            n_bootstrap=n_bootstrap,
        )

        print("\nComputing bootstrap confidence intervals...")
        selection_accuracies = []

        names = []
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                print(f"{reconstructor.get_assignment_name()}: Ground Truth (skipping)")
                continue
            mean_acc, lower, upper = self._bootstrap_selection_accuracy(i, config)
            selection_accuracies.append((mean_acc, lower, upper))
            print(
                f"{reconstructor.get_assignment_name()}: {mean_acc:.4f} "
                f"[{lower:.4f}, {upper:.4f}]"
            )
            names.append(reconstructor.get_assignment_name())

        return SelectionAccuracyPlotter.plot_selection_accuracies(
            names, selection_accuracies, config
        )

    # ==================== Neutrino Deviation Methods ====================

    def evaluate_neutrino_deviation(
        self,
        reconstructor_index: int,
        per_event: bool = False,
        deviation_type: str = "relative",
    ) -> Union[float, np.ndarray]:
        """
        Evaluate neutrino reconstruction deviation for a specific reconstructor.

        Args:
            reconstructor_index: Index of the reconstructor
            per_event: If True, return per-event deviation; else overall mean
            deviation_type: Type of deviation ('relative' or 'absolute')

        Returns:
            Deviation value(s)
        """
        if self.y_test.get("neutrino_truth") is None:
            raise ValueError(
                "No regression targets found in y_test. "
                "Cannot evaluate neutrino deviation."
            )

        predictions = self.prediction_manager.get_neutrino_predictions(
            reconstructor_index
        )
        true_neutrinos = self.y_test["neutrino_truth"]

        if deviation_type == "relative":
            return NeutrinoDeviationCalculator.compute_relative_deviation(
                predictions,
                true_neutrinos,
                per_event=per_event,
            )
        elif deviation_type == "absolute":
            return NeutrinoDeviationCalculator.compute_absolute_deviation(
                predictions,
                true_neutrinos,
                per_event=per_event,
            )
        else:
            raise ValueError(
                f"Unknown deviation_type: {deviation_type}. "
                "Must be 'relative' or 'absolute'."
            )

    def _bootstrap_neutrino_deviation(
        self,
        reconstructor_index: int,
        config: PlotConfig,
        deviation_type: str = "relative",
    ) -> Tuple[float, float, float]:
        """Compute neutrino deviation with bootstrap confidence intervals."""
        deviation_data = self.evaluate_neutrino_deviation(
            reconstructor_index, per_event=True, deviation_type=deviation_type
        )
        return BootstrapCalculator.compute_bootstrap_ci(
            deviation_data,
            n_bootstrap=config.n_bootstrap,
            confidence=config.confidence,
        )

    def plot_all_neutrino_deviations(
        self,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        figsize: Tuple[int, int] = (10, 6),
        deviation_type: str = "relative",
    ):
        """
        Plot neutrino deviations for all reconstructors with error bars.

        Args:
            n_bootstrap: Number of bootstrap samples
            confidence: Confidence level for intervals
            figsize: Figure size
            deviation_type: Type of deviation ('relative' or 'absolute')

        Returns:
            Tuple of (figure, axis)
        """
        if self.y_test.get("neutrino_truth") is None:
            raise ValueError(
                "No regression targets found in y_test. "
                "Cannot evaluate neutrino deviation."
            )

        config = PlotConfig(
            figsize=figsize,
            confidence=confidence,
            n_bootstrap=n_bootstrap,
        )

        print("\nComputing bootstrap confidence intervals for neutrino deviation...")
        deviations = []
        names = []

        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                print(f"{reconstructor.get_full_reco_name()}: Ground Truth (skipping)")
                # continue

            # Check if reconstructor supports neutrino reconstruction
            neutrino_pred = self.prediction_manager.get_neutrino_predictions(i)
            if neutrino_pred is None:
                print(
                    f"{reconstructor.get_full_reco_name()}: No neutrino reconstruction (skipping)"
                )
                continue

            mean_dev, lower, upper = self._bootstrap_neutrino_deviation(
                i, config, deviation_type
            )
            deviations.append((mean_dev, lower, upper))
            print(
                f"{reconstructor.get_full_reco_name()}: {mean_dev:.4f} "
                f"[{lower:.4f}, {upper:.4f}]"
            )
            names.append(reconstructor.get_full_reco_name())

        if not deviations:
            raise ValueError(
                "No reconstructors with neutrino reconstruction found. "
                "Cannot plot neutrino deviations."
            )

        return NeutrinoDeviationPlotter.plot_overall_deviations(
            names, deviations, config
        )

    # ==================== Binned Accuracy Methods ====================

    def plot_binned_accuracy(
        self,
        feature_data_type: str,
        feature_name: str,
        fancy_feature_label: Optional[str] = None,
        bins: int = 20,
        xlims: Optional[Tuple[float, float]] = None,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        show_errorbar: bool = True,
        show_combinatoric: bool = True,
    ):
        """Plot binned accuracy vs. a feature with bootstrap error bars."""
        config = PlotConfig(
            confidence=confidence,
            n_bootstrap=n_bootstrap,
            show_errorbar=show_errorbar,
        )

        # Extract feature data
        feature_data = FeatureExtractor.extract_feature(
            self.X_test,
            self.config.feature_indices,
            feature_data_type,
            feature_name,
        )

        # Create bins
        bin_edges = BinningUtility.create_bins(feature_data, bins, xlims)
        binning_mask = BinningUtility.create_binning_mask(feature_data, bin_edges)
        bin_centers = BinningUtility.compute_bin_centers(bin_edges)

        # Get event weights
        event_weights = FeatureExtractor.get_event_weights(self.X_test)

        # Compute combinatoric baseline if requested
        combinatoric_accuracy = None
        if show_combinatoric:
            combinatoric_per_event = AccuracyCalculator.compute_combinatoric_baseline(
                self.X_test, self.config.padding_value
            )
            combinatoric_accuracy = BinningUtility.compute_weighted_binned_statistic(
                binning_mask, combinatoric_per_event, event_weights
            )

        # Compute binned accuracies for each reconstructor
        print(f"\nComputing binned accuracy for {feature_name}...")
        binned_accuracies = []
        names = []
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                continue
            accuracy_data = self.evaluate_accuracy(i, per_event=True)

            if show_errorbar:
                mean_acc, lower, upper = BootstrapCalculator.compute_binned_bootstrap(
                    binning_mask,
                    event_weights,
                    accuracy_data,
                    config.n_bootstrap,
                    config.confidence,
                )
                binned_accuracies.append((mean_acc, lower, upper))
            else:
                binned_acc = BinningUtility.compute_weighted_binned_statistic(
                    binning_mask, accuracy_data, event_weights
                )
                binned_accuracies.append((binned_acc, binned_acc, binned_acc))
            names.append(reconstructor.get_assignment_name())

        bin_counts = np.sum(
            event_weights.reshape(1, -1) * binning_mask, axis=1
        ) / np.sum(event_weights)

        feature_label = fancy_feature_label or feature_name

        return AccuracyPlotter.plot_binned_accuracy(
            bin_centers,
            binned_accuracies,
            names,
            bin_counts,
            bin_edges,
            feature_label,
            config,
            show_combinatoric,
            combinatoric_accuracy,
        )

    def plot_binned_selection_accuracy(
        self,
        feature_data_type: str,
        feature_name: str,
        fancy_feature_label: Optional[str] = None,
        bins: int = 20,
        xlims: Optional[Tuple[float, float]] = None,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        show_errorbar: bool = True,
        show_combinatoric: bool = True,
    ):
        """Plot binned accuracy vs. a feature with bootstrap error bars."""
        config = PlotConfig(
            confidence=confidence,
            n_bootstrap=n_bootstrap,
            show_errorbar=show_errorbar,
        )

        feature_data = FeatureExtractor.extract_feature(
            self.X_test,
            self.config.feature_indices,
            feature_data_type,
            feature_name,
        )

        bin_edges = BinningUtility.create_bins(feature_data, bins, xlims)
        binning_mask = BinningUtility.create_binning_mask(feature_data, bin_edges)
        bin_centers = BinningUtility.compute_bin_centers(bin_edges)

        event_weights = FeatureExtractor.get_event_weights(self.X_test)

        combinatoric_accuracy = None
        if show_combinatoric:
            combinatoric_per_event = (
                SelectionAccuracyCalculator.compute_combinatoric_baseline(
                    self.X_test, self.config.padding_value
                )
            )
            combinatoric_accuracy = BinningUtility.compute_weighted_binned_statistic(
                binning_mask, combinatoric_per_event, event_weights
            )

        print(f"\nComputing binned accuracy for {feature_name}...")
        binned_accuracies = []
        names = []
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                continue
            accuracy_data = self.evaluate_selection_accuracy(i, per_event=True)

            if show_errorbar:
                mean_acc, lower, upper = BootstrapCalculator.compute_binned_bootstrap(
                    binning_mask,
                    event_weights,
                    accuracy_data,
                    config.n_bootstrap,
                    config.confidence,
                )
                binned_accuracies.append((mean_acc, lower, upper))
            else:
                binned_acc = BinningUtility.compute_weighted_binned_statistic(
                    binning_mask, accuracy_data, event_weights
                )
                binned_accuracies.append((binned_acc, binned_acc, binned_acc))
            names.append(reconstructor.get_assignment_name())

        bin_counts = np.sum(
            event_weights.reshape(1, -1) * binning_mask, axis=1
        ) / np.sum(event_weights)

        feature_label = fancy_feature_label or feature_name

        return SelectionAccuracyPlotter.plot_binned_selection_accuracy(
            bin_centers,
            binned_accuracies,
            names,
            bin_counts,
            bin_edges,
            feature_label,
            config,
            show_combinatoric,
            combinatoric_accuracy,
        )

    def plot_confusion_matrices(
        self,
        normalize: bool = True,
        figsize_per_plot: Tuple[int, int] = (5, 5),
    ):
        """Plot confusion matrices for all reconstructors."""
        predictions_list = [
            self.prediction_manager.get_assignment_predictions(i)
            for i in range(len(self.prediction_manager.reconstructors))
            if not isinstance(
                self.prediction_manager.reconstructors[i],
                GroundTruthReconstructor,
            )
        ]
        names = [
            r.get_assignment_name()
            for r in self.prediction_manager.reconstructors
            if not isinstance(r, GroundTruthReconstructor)
        ]

        return ConfusionMatrixPlotter.plot_confusion_matrices(
            self.y_test["assignment_labels"],
            predictions_list,
            names,
            normalize,
            figsize_per_plot,
        )

    # ==================== Complementarity Methods ====================

    def compute_complementarity_matrix(self) -> np.ndarray:
        """Compute complementarity matrix between reconstructors."""
        return ComplementarityAnalyzer.compute_complementarity_matrix()

    def plot_complementarity_matrix(
        self,
        figsize: Tuple[int, int] = (8, 6),
    ):
        fig, ax = plt.subplots(figsize=figsize)
        """Plot complementarity matrix between reconstructors."""
        matrix = self.compute_complementarity_matrix()
        names = [
            r.get_assignment_name()
            for r in self.prediction_manager.reconstructors
            if not isinstance(r, GroundTruthReconstructor)
        ]
        return sns.heatmap(
            matrix,
            annot=True,
            fmt=".2f",
            xticklabels=names,
            yticklabels=names,
            cmap="viridis",
            cbar_kws={"label": "Complementarity"},
            ax=ax,
        )


    def plot_binned_reco_resolution(
        self,
        feature_data_type: str,
        feature_name: str,
        variable_name: str,
        ylabel: str,
        fancy_feature_label: Optional[str] = None,
        bins: int = 20,
        xlims: Optional[Tuple[float, float]] = None,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        show_errorbar: bool = True,
        statistic: str = "std",
        use_signed_deviation: bool = True,
        use_relative_deviation: bool = False,
        combine_tops: bool = False,
    ):
        """
        Plot binned resolution or deviation of any reconstructed variable vs. a feature.

        Args:
            feature_data_type: Type of feature data ('jet', 'lepton', 'met', etc.)
            feature_name: Name of the feature to bin by
            variable_func: Function that takes (top1_p4, top2_p4, lepton_features, jet_features, neutrino_pred)
                          and returns reconstructed variable(s)
            truth_extractor: Function to extract truth values from (X_test, feature_indices)
            ylabel: Y-axis label for the plot
            fancy_feature_label: Optional fancy label for the feature
            bins: Number of bins
            xlims: Optional x-axis limits
            n_bootstrap: Number of bootstrap samples
            confidence: Confidence level for intervals
            show_errorbar: Whether to show error bars
            statistic: Statistic to compute ('std' for resolution, 'mean' for deviation)
            combine_tops: If True, combine results from both tops (for per-top quantities)
            use_signed_deviation: If True, use signed deviation instead of absolute

        Returns:
            Tuple of (figure, axis)
        """
        config = PlotConfig(
            confidence=confidence,
            n_bootstrap=n_bootstrap,
            show_errorbar=show_errorbar,
        )

        # Extract feature data and create bins
        feature_data = FeatureExtractor.extract_feature(
            self.X_test,
            self.config.feature_indices,
            feature_data_type,
            feature_name,
        )

        bin_edges = BinningUtility.create_bins(feature_data, bins, xlims)
        binning_mask = BinningUtility.create_binning_mask(feature_data, bin_edges)
        bin_centers = BinningUtility.compute_bin_centers(bin_edges)
        event_weights = FeatureExtractor.get_event_weights(self.X_test)

        # Compute metric for each reconstructor
        print(f"\nComputing binned {ylabel} for {feature_name}...")
        binned_metrics = []

        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            # Compute reconstructed and truth values
            reconstructed = self.variable_handler.compute_reconstructed_variable(
                i, variable_name
            )
            truth = self.variable_handler.compute_true_variable(variable_name)

            # Compute deviation
            deviation = ResolutionCalculator.compute_deviation(
                reconstructed,
                truth,
                use_signed_deviation=use_signed_deviation,
                use_relative_deviation=use_relative_deviation,
            )

            # Extend binning mask and weights if combining tops
            if combine_tops:
                binning_mask_ext = np.concatenate([binning_mask, binning_mask], axis=1)
                event_weights_ext = np.concatenate([event_weights, event_weights])
            else:
                binning_mask_ext = binning_mask
                event_weights_ext = event_weights

            if show_errorbar:
                mean_metric, lower, upper = (
                    BootstrapCalculator.compute_binned_bootstrap(
                        binning_mask_ext,
                        event_weights_ext,
                        deviation,
                        config.n_bootstrap,
                        config.confidence,
                        statistic=statistic,
                    )
                )
                binned_metrics.append((mean_metric, lower, upper))
            else:
                metric = BinningUtility.compute_weighted_binned_statistic(
                    binning_mask_ext,
                    deviation,
                    event_weights_ext,
                    statistic=statistic,
                )
                binned_metrics.append((metric, metric, metric))

        # Compute bin counts
        bin_counts = np.sum(
            event_weights.reshape(1, -1) * binning_mask, axis=1
        ) / np.sum(event_weights)

        # Plot
        feature_label = fancy_feature_label or feature_name
        names = [r.get_full_reco_name() for r in self.prediction_manager.reconstructors]

        return ResolutionPlotter.plot_binned_resolution(
            bin_centers,
            binned_metrics,
            names,
            bin_counts,
            bin_edges,
            feature_label,
            ylabel,
            config,
        )

    def plot_reco_vs_truth_distribution(
        self,
        ax,
        reconstructor_index: int,
        variable_name: str,
        variable_label: str,
        bins: int = 50,
        xlims: Optional[Tuple[float, float]] = None,
        combine_tops: bool = False,
    ):
        """
        Plot distribution of reconstructed variable vs. truth.

        Args:
            reconstructor_index: Index of the reconstructor
            variable_func: Function that takes (top1_p4, top2_p4, lepton_features, jet_features, neutrino_pred)
                          and returns reconstructed variable(s)
            truth_extractor: Function to extract truth values from X_test
            variable_label: Label for the variable being plotted
            bins: Number of bins
            xlims: Optional x-axis limits
            figsize: Figure size

        Returns:
            Tuple of (figure, axis)
        """
        # Compute reconstructed and truth values
        reconstructed = self.variable_handler.compute_reconstructed_variable(
            reconstructor_index, variable_name
        )
        truth = self.variable_handler.compute_true_variable(variable_name)

        event_weights = FeatureExtractor.get_event_weights(self.X_test)

        if combine_tops:
            event_weights = np.concatenate([event_weights, event_weights])
        return DistributionPlotter.plot_feature_distributions(
            [reconstructed, truth],
            variable_label,
            event_weights=event_weights,
            bins=bins,
            xlims=xlims,
            labels=["reco", "truth"],
            ax=ax,
        )


    def plot_deviations_distributions_all_reconstructors(
        self,
        variable_name: str,
        variable_label: str,
        figsize: Optional[Tuple[int, int]] = (10, 10),
        use_signed_deviation: bool = True,
        use_relative_deviation: bool = False,
        deviation_function: Optional[Callable] = None,
        **kwargs,
    ):
        """
        Plot distributions of deviations for all reconstructors.

        Args:
            variable_func: Function that computes the variable from (leptons, jets, neutrinos)
            truth_extractor: Function that extracts truth values from X_test
            variable_label: Label for the variable being plotted
            xlims: Optional x-axis limits
            bins: Number of bins
            figsize: Figure size
        Returns:
            Tuple of (figure, axes)
        """
        fig, axes = plt.subplots(
            figsize=figsize,
        )

        # Collect all deviations and labels
        all_deviations = []
        labels = []
        event_weights = FeatureExtractor.get_event_weights(self.X_test)

        # Extract common parameters from kwargs
        combine_tops = kwargs.pop("combine_tops", False)

        for reco_index, reconstructor in enumerate(
            self.prediction_manager.reconstructors
        ):

            # Compute reconstructed and truth values
            reconstructed = self.variable_handler.compute_reconstructed_variable(
                reco_index, variable_name
            )
            truth = self.variable_handler.compute_true_variable(variable_name)

            # Compute deviation
            deviation = ResolutionCalculator.compute_deviation(
                reconstructed,
                truth,
                use_signed_deviation=use_signed_deviation,
                use_relative_deviation=use_relative_deviation,
                deviation_function=deviation_function,
            )

            all_deviations.append(deviation)
            labels.append(reconstructor.get_full_reco_name())

        # Handle combined tops for event weights
        if combine_tops:
            event_weights_plot = np.concatenate([event_weights, event_weights])
        else:
            event_weights_plot = event_weights

        # Plot all deviations together
        DistributionPlotter.plot_feature_distributions(
            all_deviations,
            f"{variable_label}",
            event_weights=event_weights_plot,
            labels=labels,
            ax=axes,
            **kwargs,
        )

        axes.set_title(f"{variable_label} Deviation for all Reconstructors")

        return fig, axes

    def plot_distributions_all_reconstructors(
        self,
        variable_name: str,
        variable_label: str,
        xlims: Optional[Tuple[float, float]] = None,
        bins: int = 50,
        figsize: Optional[Tuple[int, int]] = (6, 5),
        save_individual_plots: bool = False,
        **kwargs,
    ):
        """
        Plot distributions for all reconstructors and truth.

        Args:
            variable_func: Function that computes the variable from (leptons, jets, neutrinos)
            truth_extractor: Function that extracts truth values from X_test
            variable_label: Label for the variable being plotted
            xlims: Optional x-axis limits
            bins: Number of bins
            figsize: Figure size

        Returns:
            Tuple of (figure, axes)
        """
        num_plots = len(self.prediction_manager.reconstructors)  # Exclude ground truth

        num_cols = np.ceil(np.sqrt(num_plots)).astype(int)
        num_rows = np.ceil(num_plots / num_cols).astype(int)

        if not save_individual_plots:
            fig, axes = plt.subplots(
                num_rows,
                num_cols,
                figsize=(figsize[0] * num_cols, figsize[1] * num_rows),
                constrained_layout=True,
            )
            if isinstance(axes, np.ndarray):
                axes = axes.flatten()
            else:
                axes = [axes]
        else:
            fig, axes = [], []
            for i in range(num_plots):
                fig_i, ax_i = plt.subplots(
                    figsize=figsize,
                    constrained_layout=True,
                )
                fig.append(fig_i)
                axes.append(ax_i)

        for reco_index, reconstructor in enumerate(
            self.prediction_manager.reconstructors
        ):
            #            if isinstance(reconstructor, GroundTruthReconstructor):
            #                continue
            ax = axes[reco_index]
            self.plot_reco_vs_truth_distribution(
                ax,
                reco_index,
                variable_name,
                variable_label,
                bins=bins,
                xlims=xlims,
                **kwargs,
            )
            ax.set_title(reconstructor.get_full_reco_name())

        if not save_individual_plots:
            for i in range(len(self.prediction_manager.reconstructors), len(axes)):
                fig.delaxes(axes[i])  # Remove unused subplots

        return fig, axes

    # ==================== Plot Specific Variable distributions ====================

    def plot_variable_distribution(self, variable_key: str, **kwargs):
        """Generic method to plot variable distributions using configuration."""
        config = self.variable_configs[variable_key]

        return self.plot_distributions_all_reconstructors(
            variable_key,
            variable_label=config["label"],
            combine_tops=config.get("combine_tops", False),
            **kwargs,
        )

    # ==================== Deviation Distribution Methods ====================

    def plot_variable_deviation(self, variable_key: str, **kwargs):
        """Generic method to plot variable deviations using configuration."""
        config = self.variable_configs[variable_key]
        # Set defaults from config, allow kwargs to override
        defaults = {
            "use_relative_deviation": config.get("use_relative_deviation", False),
            "combine_tops": config.get("combine_tops", False),
            "deviation_function": config.get("deviation_function", None)
        }
        if "deviation_label" in config:
            variable_label = config["deviation_label"]
        else:
            label = config["label"]
            variable_label = f"Relative Deviation in {label}" if config["use_relative_deviation"] else f"Deviation in {label}"

        defaults.update(kwargs)

        return self.plot_deviations_distributions_all_reconstructors(
            variable_name=variable_key,
            variable_label=variable_label,
            **defaults,
        )

    def plot_variable_confusion_matrix(
        self,
        variable_key: str,
        **kwargs,
    ):
        """Generic method to plot variable confusion matrices using configuration."""
        if variable_key not in self.variable_configs:
            raise ValueError(
                f"Variable key '{variable_key}' not found in configurations."
            )

        return self.plot_variable_confusion_matrix_for_all_reconstructors(
            variable_name=variable_key,
            variable_label=f"{self.variable_configs[variable_key]['label']}",
            **kwargs,
        )

    def plot_variable_confusion_matrix_for_all_reconstructors(
        self,
        variable_name: str,
        variable_label: str,
        bins: int = 10,
        xlims: Optional[Tuple[float, float]] = None,
        figsize_per_plot: Tuple[int, int] = (5, 5),
        normalize: str = "true",
        **kwargs,
    ):
        """Plot confusion matrices for all reconstructors for a specific variable."""
        names = []
        truth = self.variable_handler.compute_true_variable(variable_name)
        reconstructed_list = []
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if (
                isinstance(reconstructor, GroundTruthReconstructor)
                and not reconstructor.use_nu_flows
                and not reconstructor.perform_regression
            ):
                continue

            # Compute reconstructed and truth values
            reconstructed_list.append(
                self.variable_handler.compute_reconstructed_variable(i, variable_name)
            )
            names.append(reconstructor.get_full_reco_name())

        if xlims is None:
            xlims = np.min(np.concatenate([*reconstructed_list, truth])), np.max(
                np.concatenate([*reconstructed_list, truth])
            )

        # Digitize into bins
        bin_edges = np.linspace(
            xlims[0],
            xlims[1],
            bins + 1,
        )
        num_plots = len(self.prediction_manager.reconstructors)  # Exclude ground truth
        num_cols = np.ceil(np.sqrt(num_plots)).astype(int)
        num_rows = np.ceil(num_plots / num_cols).astype(int)
        fig, axes = plt.subplots(
            num_rows,
            num_cols,
            figsize=(figsize_per_plot[0] * num_cols, figsize_per_plot[1] * num_rows),
            constrained_layout=True,
        )
        if isinstance(axes, np.ndarray):
            axes = axes.flatten()
        else:
            axes = [axes]

        for reco_index, reconstructed, name in zip(
            range(len(reconstructed_list)), reconstructed_list, names
        ):
            ConfusionMatrixPlotter.plot_variable_confusion_matrix(
                truth,
                reconstructed,
                variable_label,
                axes[reco_index],
                bin_edges,
                normalize=normalize,
                **kwargs,
            )
            # Add correlation coefficient to axis
            corr = np.corrcoef(truth.flatten(), reconstructed.flatten())[0, 1]
            axes[reco_index].text(
                0.05,
                0.95,
                f"œÅ = {corr:.3f}",
                transform=axes[reco_index].transAxes,
                verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="white", alpha=0.8),
            )
            axes[reco_index].set_title(name)
        for i in range(len(reconstructed_list), len(axes)):
            fig.delaxes(axes[i])  # Remove unused subplots
        return fig, axes

    # ==================== Binned Variable Resolution/Deviation Methods ====================

    def plot_binned_variable(
        self,
        variable_key: str,
        metric_type: str,
        feature_data_type: str,
        feature_name: str,
        **kwargs,
    ):
        """Generic method to plot binned metrics using configuration.

        Args:
            variable_key: Key identifying the variable (e.g., 'top_mass', 'c_han')
            metric_type: Either 'resolution' or 'deviation'
            feature_data_type: Type of feature data for binning
            feature_name: Name of feature for binning
            **kwargs: Additional arguments passed to plot_binned_reco_resolution
        """
        config = self.variable_configs[variable_key]
        resolution_config = config.get("resolution", {})

        # Determine parameters based on metric type
        if metric_type == "resolution":
            statistic = "std"
            use_signed_deviation = False
            ylabel_template = resolution_config.get(
                "ylabel_resolution", f"{config['label']} Resolution"
            )
        else:  # deviation
            statistic = "mean"
            use_signed_deviation = True
            ylabel_template = resolution_config.get(
                "ylabel_deviation", f"Mean {config['label']} Deviation"
            )

        # Get defaults from config
        defaults = {
            "statistic": statistic,
            "use_signed_deviation": use_signed_deviation,
            "use_relative_deviation": resolution_config.get(
                "use_relative_deviation", True
            ),
            "combine_tops": config.get("combine_tops", False),
        }
        defaults.update(kwargs)

        return self.plot_binned_reco_resolution(
            feature_data_type=feature_data_type,
            feature_name=feature_name,
            variable_name=variable_key,
            ylabel=ylabel_template,
            **defaults,
        )

    # ==================== Variable Configuration and Computation Methods ====================

    def save_accuracy_latex_table(
        self,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
        save_dir: Optional[str] = None,
    ) -> str:
        """
        Generate LaTeX table with accuracy and selection accuracy for all reconstructors.

        Args:
            n_bootstrap: Number of bootstrap samples for confidence intervals
            confidence: Confidence level for intervals
            caption: Table caption
            label: Table label for referencing

        Returns:
            LaTeX table string
        """
        config = PlotConfig(
            n_bootstrap=n_bootstrap,
            confidence=confidence,
        )

        # Collect results
        results = []
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                continue

            name = reconstructor.get_assignment_name()

            # Compute accuracy with CI
            acc_mean, acc_lower, acc_upper = self._bootstrap_accuracy(i, config)

            # Compute selection accuracy with CI
            sel_acc_mean, sel_acc_lower, sel_acc_upper = (
                self._bootstrap_selection_accuracy(i, config)
            )

            results.append(
                {
                    "name": name,
                    "accuracy": (acc_mean, acc_lower, acc_upper),
                    "selection_accuracy": (sel_acc_mean, sel_acc_lower, sel_acc_upper),
                }
            )

        # Generate LaTeX table
        latex = []
        latex.append(r"    \begin{tabular}{lcc}")
        latex.append(r"        \toprule")
        latex.append(r"        Method & Assignment Accuracy & Selection Accuracy \\")
        latex.append(r"        \midrule")

        for res in results:
            name = res["name"]
            acc_mean, acc_lower, acc_upper = res["accuracy"]
            sel_mean, sel_lower, sel_upper = res["selection_accuracy"]

            acc_str = (
                f"${acc_mean:.4f}"
                + "_{-"
                + f"{acc_mean - acc_lower:.4f}"
                + "}"
                + "^{+"
                + f"{acc_upper - acc_mean:.4f}"
                + "}$"
            )
            sel_str = (
                f"${sel_mean:.4f}"
                + "_{-"
                + f"{sel_mean - sel_lower:.4f}"
                + "}"
                + "^{+"
                + f"{sel_upper - sel_mean:.4f}"
                + "}$"
            )

            latex.append(f"        {name} & {acc_str} & {sel_str} \\\\")

        latex.append(r"        \bottomrule")
        latex.append(r"    \end{tabular}")

        latex_str = "\n".join(latex)
        file_name = "reconstruction_accuracies_table.tex"
        if save_dir is not None:
            file_name = os.path.join(save_dir, file_name)
        with open(file_name, "w") as f:
            f.write(latex_str)
        print(f"LaTeX table saved to {file_name}")

    def plot_binned_accuracy_quotients(
        self,
        feature_data_type: str,
        feature_name: str,
        fancy_feature_label: Optional[str] = None,
        bins: int = 20,
        xlims: Optional[Tuple[float, float]] = None,
        n_bootstrap: int = 1,
        confidence: float = 0.95,
    ):
        """
        Plot binned quotient of assignment accuracy / selection accuracy vs. a feature.

        The quotient indicates how well the assignment performs relative to just
        selecting the correct jets (regardless of assignment to leptons).

        Args:
            feature_data_type: Type of feature data ('jet', 'lepton', 'met', etc.)
            feature_name: Name of the feature to bin by
            fancy_feature_label: Optional fancy label for the feature
            bins: Number of bins
            xlims: Optional x-axis limits
            n_bootstrap: Number of bootstrap samples
            confidence: Confidence level for intervals
            show_errorbar: Whether to show error bars

        Returns:
            Tuple of (figure, axis)
        """
        config = PlotConfig(
            confidence=confidence,
            n_bootstrap=n_bootstrap,
            show_errorbar=True,
        )

        # Extract feature data
        feature_data = FeatureExtractor.extract_feature(
            self.X_test,
            self.config.feature_indices,
            feature_data_type,
            feature_name,
        )

        # Create bins
        bin_edges = BinningUtility.create_bins(feature_data, bins, xlims)
        binning_mask = BinningUtility.create_binning_mask(feature_data, bin_edges)
        bin_centers = BinningUtility.compute_bin_centers(bin_edges)

        # Get event weights
        event_weights = FeatureExtractor.get_event_weights(self.X_test)

        # Compute binned quotients for each reconstructor
        print(f"\nComputing binned accuracy quotients for {feature_name}...")
        binned_quotients = []
        names = []

        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            if isinstance(reconstructor, GroundTruthReconstructor):
                continue

            # Get per-event accuracies
            assignment_acc = self.evaluate_accuracy(i, per_event=True)
            selection_acc = self.evaluate_selection_accuracy(i, per_event=True)

            mean_quotient, lower, upper = (
                BootstrapCalculator.compute_binned_function_bootstrap(
                    binning_mask,
                    event_weights,
                    (assignment_acc, selection_acc),
                    lambda x, y: np.divide(x, y, out=np.zeros_like(x), where=y != 0),
                    config.n_bootstrap,
                    config.confidence,
                    statistic="mean",
                )
            )
            binned_quotients.append((mean_quotient, lower, upper))

            names.append(reconstructor.get_assignment_name())

        # Compute bin counts
        bin_counts = np.sum(
            event_weights.reshape(1, -1) * binning_mask, axis=1
        ) / np.sum(event_weights)

        # Plot
        feature_label = fancy_feature_label or feature_name
        fig, ax = plt.subplots(figsize=(10, 6))
        color_map = plt.get_cmap("tab10")
        fmt_map = ["o", "s", "D", "^", "v", "P", "*", "X", "h", "8"]

        for i, (name, (mean, lower, upper)) in enumerate(zip(names, binned_quotients)):
            ax.errorbar(
                bin_centers,
                mean,
                yerr=[mean - lower, upper - mean],
                fmt=fmt_map[i % len(fmt_map)],
                color=color_map(i % 10),
                label=name,
                linestyle="None",
            )

        ax.plot(
            bin_centers,
            0.5 * np.ones_like(bin_centers),
            linestyle="--",
            color="black",
            label="Random Assignment",
        )

        ax.set_xlabel(feature_label)
        ax.set_ylabel("Assignment Accuracy / Selection Accuracy")
        ax.legend()
        ax.grid(alpha=0.3)
        ax.set_title(
            f"Binned Accuracy Quotients vs {feature_label} ({config.confidence*100:.0f}% CI)"
        )
        AccuracyPlotter._add_count_histogram(ax, bin_centers, bin_counts, bin_edges)
        plt.tight_layout()

        return fig, ax

    def plot_relative_neutrino_deviations(
        self, bins=10, xlims=None, coords="cartesian"
    ):
        """
        Plot deviation distributions for magnitude and direction of neutrino momenta

        :param bins: Number of bins
        :param xlims: Optional x-axis limits
        """
        true_neutrino = self.y_test["neutrino_truth"]
        event_weights = FeatureExtractor.get_event_weights(self.X_test)
        neutrino_deviations = []
        names = []
        nu_flows = False
        for i, reconstructor in enumerate(self.prediction_manager.reconstructors):
            names.append(reconstructor.get_full_reco_name())
            pred_neutrino = self.prediction_manager.get_neutrino_predictions(i)

            if coords == "spherical":
                true_neutrino_mag = np.linalg.norm(true_neutrino[..., :3], axis=-1)
                pred_neutrino_mag = np.linalg.norm(pred_neutrino[..., :3], axis=-1)

                mag_deviation = (
                    pred_neutrino_mag - true_neutrino_mag
                ) / true_neutrino_mag

                mag_product = true_neutrino_mag * pred_neutrino_mag

                dir_deviation = np.arccos(
                    np.clip(
                        np.divide(
                            np.sum(
                                pred_neutrino[..., :3] * true_neutrino[..., :3], axis=-1
                            ),
                            mag_product,
                            out=np.zeros_like(true_neutrino_mag),
                            where=((mag_product) != 0)
                            & (~np.isnan(mag_product) & ~np.isinf(mag_product)),
                        ),
                        -1.0,
                        1.0,
                    )
                )
                neutrino_deviations.append(np.array([mag_deviation, dir_deviation]))
            elif coords == "cartesian":
                x_deviation = (pred_neutrino[..., 0] - true_neutrino[..., 0]) / 1e3
                y_deviation = (pred_neutrino[..., 1] - true_neutrino[..., 1]) / 1e3
                z_deviation = (pred_neutrino[..., 2] - true_neutrino[..., 2]) / 1e3
                neutrino_deviations.append(
                    np.array([x_deviation, y_deviation, z_deviation])
                )
            elif coords == "spherical_lepton_fixed":
                lepton_features = self.X_test["lep_inputs"]
                lepton_3vect = lorentz_vector_from_PtEtaPhiE_array(
                    lepton_features[..., :4]
                )[..., :3]

                true_neutrino_z = project_vectors_onto_axis(
                    true_neutrino[..., :3], lepton_3vect
                )
                pred_neutrino_z = project_vectors_onto_axis(
                    pred_neutrino[..., :3], lepton_3vect
                )

                z_deviation = (pred_neutrino_z - true_neutrino_z) / 1e3
                true_neutrino_perp = true_neutrino[..., :3] - np.expand_dims(
                    true_neutrino_z, axis=-1
                ) * (
                    lepton_3vect / np.linalg.norm(lepton_3vect, axis=-1, keepdims=True)
                )
                pred_neutrino_perp = pred_neutrino[..., :3] - np.expand_dims(
                    pred_neutrino_z, axis=-1
                ) * (
                    lepton_3vect / np.linalg.norm(lepton_3vect, axis=-1, keepdims=True)
                )
                true_neutrino_perp_mag = np.linalg.norm(true_neutrino_perp, axis=-1)
                pred_neutrino_perp_mag = np.linalg.norm(pred_neutrino_perp, axis=-1)
                perp_mag_deviation = np.divide(
                    pred_neutrino_perp_mag - true_neutrino_perp_mag,
                    true_neutrino_perp_mag,
                    out=np.zeros_like(true_neutrino_perp_mag),
                    where=true_neutrino_perp_mag != 0,
                )
                perp_dot_product = np.sum(
                    true_neutrino_perp * pred_neutrino_perp, axis=-1
                )
                perp_mag_product = true_neutrino_perp_mag * pred_neutrino_perp_mag
                perp_angle_deviation = np.arccos(
                    np.clip(
                        np.divide(
                            perp_dot_product,
                            perp_mag_product,
                            out=np.zeros_like(true_neutrino_perp_mag),
                            where=(perp_mag_product != 0)
                            & (
                                ~np.isnan(perp_mag_product)
                                & ~np.isinf(perp_mag_product)
                            ),
                        ),
                        -1.0,
                        1.0,
                    )
                )
                neutrino_deviations.append(
                    np.array([perp_mag_deviation, perp_angle_deviation, z_deviation])
                )

        if coords == "spherical":
            component_labels = [
                r"$\Delta |\vec{p}| / |\vec{p}_{\text{true}}|$",
                r"$\Delta \phi$",
            ]
        elif coords == "spherical_lepton_fixed":
            component_labels = [
                r"$\Delta |\vec{p}_{\perp}| / |\vec{p}_{\perp, \text{true}}|$",
                r"$\Delta \phi_{\perp}$",
                r"$\Delta p_{z}$ [GeV]",
            ]
        elif coords == "cartesian":
            component_labels = [
                r"$\Delta p_x$ [GeV]",
                r"$\Delta p_y$ [GeV]",
                r"$\Delta p_z$ [GeV]",
            ]

        fig, ax = plt.subplots(
            figsize=(6 * len(component_labels), 5 * self.config.NUM_LEPTONS),
            ncols=len(component_labels),
            nrows=self.config.NUM_LEPTONS,
        )

        for lepton_idx in range(self.config.NUM_LEPTONS):
            for comp_idx, component in enumerate(component_labels):
                ax_i = ax[lepton_idx, comp_idx]
                DistributionPlotter.plot_feature_distributions(
                    [
                        neutrino_deviations[i][comp_idx][..., lepton_idx]
                        for i in range(len(neutrino_deviations))
                    ],
                    f"{component}" + r"$(\nu_{" + f"{lepton_idx+1}" + r"})$",
                    event_weights=event_weights,
                    labels=names,
                    bins=bins,
                    xlims=xlims[comp_idx] if xlims is not None else None,
                    ax=ax_i,
                )

        # Collect handles and labels from the first axis
        handles, labels = ax[0, 0].get_legend_handles_labels()

        # Remove individual legends from all axes
        for lepton_idx in range(self.config.NUM_LEPTONS):
            for comp_idx in range(len(component_labels)):
                ax[lepton_idx, comp_idx].get_legend().remove()
                #pass

        # Add single legend for the whole figure
        fig.legend(
            handles,
            labels,
            loc="upper center",
            bbox_to_anchor=(0.5, 1.04),
            ncol=len(names),
        )

        plt.tight_layout()
        return fig, ax
